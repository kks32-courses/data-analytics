{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "# Create a dataframe\ndf = spark.read.json(\"people.json\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>34</td><td>application_1499674741924_0038</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-kbtu.3bs2jaxxb12u1ll00rolucu4sg.ax.internal.cloudapp.net:8088/proxy/application_1499674741924_0038/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.12:30060/node/containerlogs/container_1499674741924_0038_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# Displays the content of the DataFrame to stdout\ndf.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# Print the schema in a tree format\ndf.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# Show only `name` column\ndf.select(\"name\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+\n|   name|\n+-------+\n|Michael|\n|   Andy|\n| Justin|\n+-------+"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "# Select everybody, but increment the age by 1\ndf.select(df['name'], df['age'] + 1).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+---------+\n|   name|(age + 1)|\n+-------+---------+\n|Michael|     null|\n|   Andy|       31|\n| Justin|       20|\n+-------+---------+"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "# Select people older than 21\ndf.filter(df['age'] > 21).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "# Count people by age\ndf.groupBy(\"age\").count().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----+-----+\n| age|count|\n+----+-----+\n|  19|    1|\n|null|    1|\n|  30|    1|\n+----+-----+"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "# Create or replace a temporary view\ndf.createOrReplaceTempView(\"people\")\n\nsqlDF = spark.sql(\"SELECT * FROM people\")\nsqlDF.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "from pyspark.sql import Row\n\n# Load a text file and convert each line to a Row.\nlines = sc.textFile(\"people.txt\")\nparts = lines.map(lambda l: l.split(\",\"))\npeople = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n\n# Infer the schema, and register the DataFrame as a table.\nschemaPeople = spark.createDataFrame(people)\nschemaPeople.createOrReplaceTempView(\"people\")\n\n# SQL can be run over DataFrames that have been registered as a table.\nteenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n\n# The results of SQL queries are Dataframe objects.\n# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\nteenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\nfor name in teenNames:\n    print(name)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Name: Justin"}], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "# Import data types\nfrom pyspark.sql.types import *\n\n# Load a text file and convert each line to a Row.\nlines = sc.textFile(\"people.txt\")\nparts = lines.map(lambda l: l.split(\",\"))\n\n# Each line is converted to a tuple.\npeople = parts.map(lambda p: (p[0], p[1].strip()))\n\n# The schema is encoded in a string.\nschemaString = \"name age\"\n\nfields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\nschema = StructType(fields)\n\n# Apply the schema to the RDD.\nschemaPeople = spark.createDataFrame(people, schema)\n\n# Creates a temporary view using the DataFrame\nschemaPeople.createOrReplaceTempView(\"people\")\n\n# SQL can be run over DataFrames that have been registered as a table.\nresults = spark.sql(\"SELECT name FROM people\")\n\nresults.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+\n|   name|\n+-------+\n|Michael|\n|   Andy|\n| Justin|\n+-------+"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}