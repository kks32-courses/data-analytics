{"nbformat_minor": 0, "cells": [{"source": "# Exploration and Modeling of 2013 NYC Taxi Trip and Fare Dataset on Spark 2.0 HDInsight Clusters (pySpark3)", "cell_type": "markdown", "metadata": {}}, {"source": "---------------------------------\n### Here we show key features and capabilities of Spark's MLlib toolkit using the NYC taxi trip and fare data-set from 2013 (about 40 Gb uncompressed). We take about 10% of the sample of this data (from the month of December 2013, about 3.6 Gb) to predict the amount of tip paid for each taxi trip in NYC based on features such as trip distance, number of passengers, trip distance etc. We have shown relevant plots in Python.\n\n### We have sampled the data in order for the runs to finish quickly. The same code (with minor modifications) can be run on the full 2013 data-set.\n\n### This notebook takes about 15 mins to run on a 2 worker-node cluster(D12_V2).\n\n\n### The nobebook runs in the PySpark3 kernel in Jupyter.\n----------------------------------", "cell_type": "markdown", "metadata": {}}, {"source": "### OBJECTIVE: Show use of Spark MLlib's functions for featurization and ML tasks.\n\n### The learning task is to predict the amount of tip paid in (dollars) paid for each taxi trip in December 2013\n\n#### We have shown the following steps:\n1. Data ingestion, joining, and wrangling.\n2. Data exploration and plotting.\n3. Data preparation (featurizing/transformation).\n4. Modeling (using incl. hyperparameter tuning with cross-validation), prediction, model persistance.\n5. Model evaluation on an independent validation data-set.\n\nThrough the above steps we highlight Spark SQL, as well as, MLlib's modeling and transformation functions.", "cell_type": "markdown", "metadata": {}}, {"source": "### Introductory material\n\nNYC 2013 Taxi data:\nhttp://www.andresmh.com/nyctaxitrips/\n\n----------------------------------", "cell_type": "markdown", "metadata": {}}, {"source": "## Set directory paths and location of training, validation files, as well as model location in blob storage\nNOTE: The blob storage attached to the HDI cluster is referenced as: wasb:/// (Windows Azure Storage Blob). Other blob storage accounts are referenced as: wasb://", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# 1. Location of training data: contains Dec 2013 trip and fare data from NYC \ntrip_file_loc = \"wasb://data@cdspsparksamples.blob.core.windows.net/NYCTaxi/KDD2016/trip_data_12.csv\"\nfare_file_loc = \"wasb://data@cdspsparksamples.blob.core.windows.net/NYCTaxi/KDD2016/trip_fare_12.csv\"\n\n# 2. Location of the joined taxi+fare training file\ntaxi_valid_file_loc = \"wasb://mllibwalkthroughs@cdspsparksamples.blob.core.windows.net/Data/NYCTaxi/JoinedTaxiTripFare.Point1Pct.Valid.csv\"\n\n# 3. Set model storage directory path. This is where models will be saved.\nmodelDir = \"wasb:///user/remoteuser/NYCTaxi/Models/\"; # The last backslash is needed;\n\n# 4. Set data storage path. This is where data is sotred on the blob attached to the cluster.\ndataDir = \"wasb:///HdiSamples/HdiSamples/NYCTaxi/\"; # The last backslash is needed;", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Set SQL context and import necessary libraries", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import *\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datetime\n\nsqlContext = SQLContext(sc)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Data ingestion and wrangling using Spark SQL", "cell_type": "markdown", "metadata": {}}, {"source": "#### Data import and registering as tables", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "## READ IN TRIP DATA FRAME FROM CSV with header and inferSchema\ntrip = \n\n## READ IN FARE DATA FRAME FROM CSV\nfare = ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "## CHECK SCHEMA OF TRIP AND FARE TABLES\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "## REGISTER DATA-FRAMEs AS A TEMP-TABLEs IN SQL-CONTEXT\n", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Using Spark SQL to join, clean and featurize data", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "## USING SQL: MERGE TRIP AND FARE DATA-SETS TO CREATE A JOINED DATA-FRAME\n## ELIMINATE SOME COLUMNS, AND FILTER ROWS WTIH VALUES OF SOME COLUMNS\nsqlStatement = \"\"\"SELECT t.medallion, t.hack_license,\n  f.total_amount, f.tolls_amount,\n  hour(f.pickup_datetime) as pickup_hour, f.vendor_id, f.fare_amount, \n  f.surcharge, f.tip_amount, f.payment_type, t.rate_code, \n  t.passenger_count, t.trip_distance, t.trip_time_in_secs \n  FROM trip t, fare f  \n  WHERE t.medallion = f.medallion AND t.hack_license = f.hack_license \n  AND t.pickup_datetime = f.pickup_datetime \n  AND t.passenger_count > 0 and t.passenger_count < 8 \n  AND f.tip_amount >= 0 AND f.tip_amount <= 25 \n  AND f.fare_amount >= 1 AND f.fare_amount <= 250 \n  AND f.tip_amount < f.fare_amount AND t.trip_distance > 0 \n  AND t.trip_distance <= 100 AND t.trip_time_in_secs >= 30 \n  AND t.trip_time_in_secs <= 7200 AND t.rate_code <= 5\n  AND f.payment_type in ('CSH','CRD')\"\"\"\n\ntrip_fareDF = \n\n# REGISTER JOINED TRIP-FARE DF IN SQL-CONTEXT\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "## SHOW WHICH TABLES ARE REGISTERED IN SQL-CONTEXT\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Sample data for creating and evaluating model & save in blob", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# SAMPLE 10% OF DATA, SPLIT INTO TRAIINING AND VALIDATION AND SAVE IN BLOB\ntrip_fare_featSampled = \ntrainfilename = dataDir + \"TrainData\";\ntrip_fare_featSampled.repartition(10).write.mode(\"overwrite\").parquet(trainfilename)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Data ingestion & cleanup: Read in the joined taxi trip and fare training file (as parquet), format and clean data, and create data-frame", "cell_type": "markdown", "metadata": {}}, {"source": "The taxi trip and fare files were joined based on the instructions provided in: \n\"https://azure.microsoft.com/en-us/documentation/articles/machine-learning-data-science-process-hive-walkthrough/\"", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "## READ IN DATA FRAME FROM CSV\ntaxi_train_df = \n\n## CREATE A CLEANED DATA-FRAME BY DROPPING SOME UN-NECESSARY COLUMNS & FILTERING FOR UNDESIRED VALUES OR OUTLIERS\ntaxi_df_train_cleaned = taxi_train_df.drop('medallion').drop('hack_license').drop('total_amount').drop('tolls_amount')\\\n    .filter(\"passenger_count > 0 and passenger_count < 8 AND tip_amount >= 0 AND tip_amount < 15 AND \\\n            fare_amount >= 1 AND fare_amount < 150 AND trip_distance > 0 AND trip_distance < 100 AND \\\n            trip_time_in_secs > 30 AND trip_time_in_secs < 7200\" )\n\n## PERSIST AND MATERIALIZE DF IN MEMORY    \ntaxi_df_train_cleaned.persist()\ntaxi_df_train_cleaned.count()\n\n## REGISTER DATA-FRAME AS A TEMP-TABLE IN SQL-CONTEXT\ntaxi_df_train_cleaned.createOrReplaceTempView(\"taxi_train\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "taxi_df_train_cleaned.printSchema()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a name=\"exploration\"></a>\n## Data exploration & visualization: Plotting of target variables and features", "cell_type": "markdown", "metadata": {}}, {"source": "#### First, summarize data using SQL, this outputs a Spark data frame. If the data-set is too large, it can be sampled", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%sql -q -o sqlResultsPD\nSELECT fare_amount, passenger_count, tip_amount FROM taxi_train WHERE passenger_count > 0 AND passenger_count < 7 AND fare_amount > 0 AND fare_amount < 100 AND tip_amount > 0 AND tip_amount < 15", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Plot histogram of tip amount, relationship between tip amount vs. other features", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n%matplotlib inline\nimport matplotlib.pyplot as plt\n## %%local creates a pandas data-frame on the head node memory, from spark data-frame, \n## which can then be used for plotting. Here, sampling data is a good idea, depending on the memory of the head node\n\n# TIP BY PAYMENT TYPE AND PASSENGER COUNT\nax1 = sqlResultsPD[['tip_amount']].plot(kind='hist', bins=25, facecolor='lightblue')\nax1.set_title('Tip amount distribution')\nax1.set_xlabel('Tip Amount ($)'); ax1.set_ylabel('Counts');\nplt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n\n# TIP BY PASSENGER COUNT\nax2 = sqlResultsPD.boxplot(column=['tip_amount'], by=['passenger_count'])\nax2.set_title('Tip amount by Passenger count')\nax2.set_xlabel('Passenger count'); ax2.set_ylabel('Tip Amount ($)');\nplt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n\n# TIP AMOUNT BY FARE AMOUNT, POINTS ARE SCALED BY PASSENGER COUNT\nax = sqlResultsPD.plot(kind='scatter', x= 'fare_amount', y = 'tip_amount', c='blue', alpha = 0.10, s=2.5*(sqlResultsPD.passenger_count))\nax.set_title('Tip amount by Fare amount')\nax.set_xlabel('Fare Amount ($)'); ax.set_ylabel('Tip Amount ($)');\nplt.axis([-2, 80, -2, 20])\nplt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a name=\"transformation\"></a>\n## Feature engineering, transformation and data preparation for modeling", "cell_type": "markdown", "metadata": {}}, {"source": "#### Create a new feature by binning hours into traffic time buckets using Spark SQL", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "### CREATE FOUR BUCKETS FOR TRAFFIC TIMES\nsqlStatement = \"\"\"SELECT payment_type, pickup_hour, fare_amount, tip_amount, \n    vendor_id, rate_code, passenger_count, trip_distance, trip_time_in_secs, \n  CASE\n    WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN 'Night'\n    WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN 'AMRush' \n    WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN 'Afternoon'\n    WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN 'PMRush'\n    END as TrafficTimeBins,\n  CASE\n    WHEN (tip_amount > 0) THEN 1 \n    WHEN (tip_amount <= 0) THEN 0 \n    END as tipped\n  FROM taxi_train\"\"\"\n\ntaxi_df_train_with_newFeatures = spark.sql(sqlStatement)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Indexing of categorical features\nHere we only transform some variables to show examples, which are character strings. Other variables, such as week-day, which are represented by numerical valies, can also be indexed as categorical variables.\n\nFor indexing, we used stringIndexer function from MLlib.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n\n# DEFINE THE TRANSFORMATIONS THAT NEEDS TO BE APPLIED TO SOME OF THE FEATURES\nsI1 = StringIndexer(inputCol=\"vendor_id\", outputCol=\"vendorIndex\");\nsI2 = StringIndexer(inputCol=\"rate_code\", outputCol=\"rateIndex\");\nsI3 = StringIndexer(inputCol=\"payment_type\", outputCol=\"paymentIndex\");\nsI4 = StringIndexer(inputCol=\"TrafficTimeBins\", outputCol=\"TrafficTimeBinsIndex\");\n\n# APPLY TRANSFORMATIONS\nencodedFinal = Pipeline(stages=[sI1, sI2, sI3, sI4]).fit(taxi_df_train_with_newFeatures).transform(taxi_df_train_with_newFeatures);", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Split data into train/test. Training fraction will be used to create model, and testing fraction will be used to evaluate model.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "trainingFraction = 0.75; testingFraction = (1-trainingFraction);\nseed = 1234;\n\n# SPLIT SAMPLED DATA-FRAME INTO TRAIN/TEST, WITH A RANDOM COLUMN ADDED FOR DOING CV (SHOWN LATER)\ntrainData, testData = encodedFinal.randomSplit([trainingFraction, testingFraction], seed=seed);\n\n# CACHE DATA FRAMES IN MEMORY\ntrainData.persist(); trainData.count()\ntestData.persist(); testData.count()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Train a regression model: Predict the amount of tip paid for taxi trips", "cell_type": "markdown", "metadata": {}}, {"source": "### Train Elastic Net regression model, and evaluate performance on test data", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml.feature import RFormula\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\n## DEFINE REGRESSION FURMULA\nregFormula = RFormula(formula=\"tip_amount ~ paymentIndex + vendorIndex + rateIndex + TrafficTimeBinsIndex + pickup_hour + passenger_count + trip_time_in_secs + trip_distance + fare_amount\")\n\n## DEFINE INDEXER FOR CATEGORIAL VARIABLES\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32)\n\n## DEFINE ELASTIC NET REGRESSOR\neNet = LinearRegression(featuresCol=\"indexedFeatures\", maxIter=25, regParam=0.01, elasticNetParam=0.5)\n\n## Fit model, with formula and other transformations\nmodel = \n\n## PREDICT ON TEST DATA AND EVALUATE\npredictions = \npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)\n\n## PLOC ACTUALS VS. PREDICTIONS\npredictions.select(\"label\",\"prediction\").createOrReplaceTempView(\"tmp_results\");", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Train Gradient Boosting Tree regression model, and evaluate performance on test data", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml.regression import GBTRegressor\n\n## DEFINE REGRESSION FURMULA\nregFormula = RFormula(formula=\"tip_amount ~ paymentIndex + vendorIndex + rateIndex + TrafficTimeBinsIndex + pickup_hour + passenger_count + trip_time_in_secs + trip_distance + fare_amount\")\n\n## DEFINE INDEXER FOR CATEGORIAL VARIABLES\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32)\n\n## DEFINE GRADIENT BOOSTING TREE REGRESSOR\ngBT = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n\n## Fit model, with formula and other transformations\nmodel = Pipeline(stages=[regFormula, featureIndexer, gBT]).fit(trainData)\n\n## PREDICT ON TEST DATA AND EVALUATE\npredictions = model.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)\n\n## PLOC ACTUALS VS. PREDICTIONS\npredictions.select(\"label\",\"prediction\").createOrReplaceTempView(\"tmp_results\");", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Train a random forest regression model using the Pipeline function, save, and evaluate on test data set", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml.feature import RFormula\nfrom sklearn.metrics import roc_curve,auc\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\n## DEFINE REGRESSION FURMULA\nregFormula = RFormula(formula=\"tip_amount ~ paymentIndex + vendorIndex + rateIndex + TrafficTimeBinsIndex + pickup_hour + passenger_count + trip_time_in_secs + trip_distance + fare_amount\")\n\n## DEFINE INDEXER FOR CATEGORIAL VARIABLES\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32)\n\n## DEFINE RANDOM FOREST ESTIMATOR\nrandForest = RandomForestRegressor(featuresCol = 'indexedFeatures', labelCol = 'label', numTrees=20, \n                                   featureSubsetStrategy=\"auto\",impurity='variance', maxDepth=6, maxBins=100)\n\n## Fit model, with formula and other transformations\nmodel = Pipeline(stages=[regFormula, featureIndexer, randForest]).fit(trainData)\n\n## SAVE MODEL\ndatestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\nfileName = \"RandomForestRegressionModel_\" + datestamp;\nrandForestDirfilename = modelDir + fileName;\nmodel.save(randForestDirfilename)\n\n## PREDICT ON TEST DATA AND EVALUATE\npredictions = model.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)\n\n## PLOC ACTUALS VS. PREDICTIONS\npredictions.select(\"label\",\"prediction\").createOrReplaceTempView(\"tmp_results\");", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%sql -q -o predictionsPD\nSELECT * from tmp_results", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "%%local\nimport numpy as np\n\nax = predictionsPD.plot(kind='scatter', figsize = (5,5), x='label', y='prediction', color='blue', alpha = 0.25, label='Actual vs. predicted');\nfit = np.polyfit(predictionsPD['label'], predictionsPD['prediction'], deg=1)\nax.set_title('Actual vs. Predicted Tip Amounts ($)')\nax.set_xlabel(\"Actual\"); ax.set_ylabel(\"Predicted\");\nax.plot(predictionsPD['label'], fit[0] * predictionsPD['label'] + fit[1], color='magenta')\nplt.axis([-1, 15, -1, 15])\nplt.show(ax)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Hyper-parameter tuning: Train a random forest model using cross-validation", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n## DEFINE RANDOM FOREST MODELS\nrandForest = RandomForestRegressor(featuresCol = 'indexedFeatures', labelCol = 'label', \n                                   featureSubsetStrategy=\"auto\",impurity='variance', maxBins=100)\n\n## DEFINE MODELING PIPELINE, INCLUDING FORMULA, FEATURE TRANSFORMATIONS, AND ESTIMATOR\npipeline = Pipeline(stages=[regFormula, featureIndexer, randForest])\n\n## DEFINE PARAMETER GRID FOR RANDOM FOREST\nparamGrid = ParamGridBuilder() \\\n    .addGrid(randForest.numTrees, [10, 25, 50]) \\\n    .addGrid(randForest.maxDepth, [3, 5, 7]) \\\n    .build()\n\n## DEFINE CROSS VALIDATION\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=RegressionEvaluator(metricName=\"rmse\"),\n                          numFolds=3)\n\n## TRAIN MODEL USING CV\ncvModel = crossval.fit(trainData)\n\n## PREDICT AND EVALUATE TEST DATA SET\npredictions = cvModel.transform(testData)\nevaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\nr2 = evaluator.evaluate(predictions)\nprint(\"R-squared on test data = %g\" % r2)\n\n## SAVE THE BEST MODEL\ndatestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\nfileName = \"CV_RandomForestRegressionModel_\" + datestamp;\nCVDirfilename = modelDir + fileName;\ncvModel.bestModel.save(CVDirfilename);", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Load a saved pipeline model and evaluate it on test data set", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml import PipelineModel\n\nsavedModel = PipelineModel.load(randForestDirfilename)\n\npredictions = savedModel.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Load and transform an independent validation data-set, and evaluate the saved pipeline model", "cell_type": "markdown", "metadata": {}}, {"source": "### Note that this validation data, by design, has a different format than the original trainig data. By grangling and transformations, we make the data format the same as the training data for the purpose of scoring.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "## READ IN DATA FRAME FROM CSV\ntaxi_valid_df = spark.read.csv(path=taxi_valid_file_loc, header=True, inferSchema=True)\ntaxi_valid_df.printSchema()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "## READ IN DATA FRAME FROM CSV\ntaxi_valid_df = spark.read.csv(path=taxi_valid_file_loc, header=True, inferSchema=True)\n\n## CREATE A CLEANED DATA-FRAME BY DROPPING SOME UN-NECESSARY COLUMNS & FILTERING FOR UNDESIRED VALUES OR OUTLIERS\ntaxi_df_valid_cleaned = taxi_valid_df.drop('medallion').drop('hack_license').drop('store_and_fwd_flag').drop('pickup_datetime')\\\n    .drop('dropoff_datetime').drop('pickup_longitude').drop('pickup_latitude').drop('dropoff_latitude')\\\n    .drop('dropoff_longitude').drop('tip_class').drop('total_amount').drop('tolls_amount').drop('mta_tax')\\\n    .drop('direct_distance').drop('surcharge')\\\n    .filter(\"passenger_count > 0 and passenger_count < 8 AND payment_type in ('CSH', 'CRD') \\\n    AND tip_amount >= 0 AND tip_amount < 30 AND fare_amount >= 1 AND fare_amount < 150 AND trip_distance > 0 \\\n    AND trip_distance < 100 AND trip_time_in_secs > 30 AND trip_time_in_secs < 7200\" )\n\n## REGISTER DATA-FRAME AS A TEMP-TABLE IN SQL-CONTEXT\ntaxi_df_valid_cleaned.createOrReplaceTempView(\"taxi_valid\")\n\n### CREATE FOUR BUCKETS FOR TRAFFIC TIMES\nsqlStatement = \"\"\" SELECT *, CASE\n     WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN \"Night\" \n     WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN \"AMRush\" \n     WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN \"Afternoon\"\n     WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN \"PMRush\"\n    END as TrafficTimeBins\n    FROM taxi_valid\n\"\"\"\ntaxi_df_valid_with_newFeatures = spark.sql(sqlStatement)\n\n## APPLY THE SAME TRANSFORATION ON THIS DATA AS ORIGINAL TRAINING DATA\nencodedFinalValid = Pipeline(stages=[sI1, sI2, sI3, sI4]).fit(taxi_df_train_with_newFeatures).transform(taxi_df_valid_with_newFeatures)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "## LOAD SAVED MODEL, SCORE VALIDATION DATA, AND EVALUATE\nsavedModel = PipelineModel.load(CVDirfilename)\npredictions = savedModel.transform(encodedFinalValid)\nr2 = evaluator.evaluate(predictions)\nprint(\"R-squared on validation data = %g\" % r2)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Save predictions to a file in HDFS", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "datestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\nfileName = \"Predictions_CV_\" + datestamp;\npredictionfile = dataDir + fileName;\npredictions.select(\"label\",\"prediction\").write.mode(\"overwrite\").csv(predictionfile)", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}, "anaconda-cloud": {}, "celltoolbar": "Raw Cell Format"}}