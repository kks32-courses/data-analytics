{"nbformat_minor": 0, "cells": [{"source": "# MLlib: Basic Statistics and Exploratory Data Analysis", "cell_type": "markdown", "metadata": {}}, {"source": "So far we have used different map and aggregation functions, on simple and key/value pair RDD's, in order to get simple statistics that help us understand our datasets. In this notebook we will introduce Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) through its basic statistics functionality in order to better understand our dataset. We will use the reduced 10-percent [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) datasets through the notebook.   ", "cell_type": "markdown", "metadata": {}}, {"source": "## Getting the data and creating the RDD", "cell_type": "markdown", "metadata": {}}, {"source": "As we did in our first notebook, we will use the reduced dataset (10 percent) provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million network interactions. The file is provided as a Gzip file that we will download locally.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Upload and load KDD Cup 99 data from `/usr/livy/`\ndata_file = \"./kddcup.data_10_percent.gz\"\nraw_data = sc.textFile(data_file)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Local vectors", "cell_type": "markdown", "metadata": {}}, {"source": "A [local vector](https://spark.apache.org/docs/latest/mllib-data-types.html#local-vector) is often used as a base type for RDDs in Spark MLlib. A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. ", "cell_type": "markdown", "metadata": {}}, {"source": "For dense vectors, MLlib uses either Python *lists* or the *NumPy* `array` type. The later is recommended, so you can simply pass NumPy arrays around.  ", "cell_type": "markdown", "metadata": {}}, {"source": "For sparse vectors, users can construct a `SparseVector` object from MLlib or pass *SciPy* `scipy.sparse` column vectors if SciPy is available in their environment. The easiest way to create sparse vectors is to use the factory methods implemented in `Vectors`.  ", "cell_type": "markdown", "metadata": {}}, {"source": "### An RDD of dense vectors", "cell_type": "markdown", "metadata": {}}, {"source": "Let's represent each network interaction in our dataset as a dense vector. For that we will use the *NumPy* `array` type.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import numpy as np\n\ndef parse_interaction(line):\n    line_split = line.split(\",\")\n    # keep just numeric and logical values\n    symbolic_indexes = # Create a list of duration (1), protocol (2), service (3) and interaction type (41) \n    clean_line_split = \n    return # Numpy array \n\n# Convert the raw data to a dense vector\nvector_data = ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Summary statistics", "cell_type": "markdown", "metadata": {}}, {"source": "Spark's MLlib provides column summary statistics for `RDD[Vector]` through the function [`colStats`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics.colStats) available in [`Statistics`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics). The method returns an instance of [`MultivariateStatisticalSummary`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.MultivariateStatisticalSummary), which contains the column-wise *max*, *min*, *mean*, *variance*, and *number of nonzeros*, as well as the *total count*.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.mllib.stat import Statistics \nfrom math import sqrt \n\n# Compute column summary statistics.\n# Print mean, SD, Max, Min, Total count and number of non zeros\nsummary = ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Summary statistics by label  ", "cell_type": "markdown", "metadata": {}}, {"source": "The interesting part of summary statistics, in our case, comes from being able to obtain them by the type of network attack or 'label' in our dataset. By doing so we will be able to better characterise our dataset dependent variable in terms of the independent variables range of values.  ", "cell_type": "markdown", "metadata": {}}, {"source": "If we want to do such a thing we could filter our RDD containing labels as keys and vectors as values. For that we just need to adapt our `parse_interaction` function to return a tuple with both elements.     ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def parse_interaction_with_key(line):\n    line_split = line.split(\",\")\n    # keep just numeric and logical values\n\n\n# Creat an RDD of interactions with key\nlabel_vector_data = ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "The next step is not very sophisticated. We use `filter` on the RDD to leave out other labels but the one we want to gather statistics from.    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Filter normal data\nnormal_label_data = ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now we can use the new RDD to call `colStats` on the values.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Create summary stats\nnormal_summary = ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "And collect the results as we did before.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "print \"Duration Statistics for label: {}\".format(\"normal\")\nprint \" Mean: {}\".format(normal_summary.mean()[0],3)\nprint \" St. deviation: {}\".format(round(sqrt(normal_summary.variance()[0]),3))\nprint \" Max value: {}\".format(round(normal_summary.max()[0],3))\nprint \" Min value: {}\".format(round(normal_summary.min()[0],3))\nprint \" Total value count: {}\".format(normal_summary.count())\nprint \" Number of non-zero values: {}\".format(normal_summary.numNonzeros()[0])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Instead of working with a key/value pair we could have just filter our raw data split using the label in column 41. Then we can parse the results as we did before. This will work as well. However having our data organised as key/value pairs will open the door to better manipulations. Since `values()` is a transformation on an RDD, and not an action, we don't perform any computation until we call `colStats` anyway.  ", "cell_type": "markdown", "metadata": {}}, {"source": "But lets wrap this within a function so we can reuse it with any label.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def summary_by_label(raw_data, label):\n  ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let's give it a try with the \"normal.\" label again.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "normal_sum = summary_by_label(raw_data, \"normal.\")\n\nprint \"Duration Statistics for label: {}\".format(\"normal\")\nprint \" Mean: {}\".format(normal_sum.mean()[0],3)\nprint \" St. deviation: {}\".format(round(sqrt(normal_sum.variance()[0]),3))\nprint \" Max value: {}\".format(round(normal_sum.max()[0],3))\nprint \" Min value: {}\".format(round(normal_sum.min()[0],3))\nprint \" Total value count: {}\".format(normal_sum.count())\nprint \" Number of non-zero values: {}\".format(normal_sum.numNonzeros()[0])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let's try now with some network attack. We have all of them listed [here](http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types).  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "guess_passwd_summary = summary_by_label(raw_data, \"guess_passwd.\")\n\nprint \"Duration Statistics for label: {}\".format(\"guess_password\")\nprint \" Mean: {}\".format(guess_passwd_summary.mean()[0],3)\nprint \" St. deviation: {}\".format(round(sqrt(guess_passwd_summary.variance()[0]),3))\nprint \" Max value: {}\".format(round(guess_passwd_summary.max()[0],3))\nprint \" Min value: {}\".format(round(guess_passwd_summary.min()[0],3))\nprint \" Total value count: {}\".format(guess_passwd_summary.count())\nprint \" Number of non-zero values: {}\".format(guess_passwd_summary.numNonzeros()[0])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We can see that this type of attack is shorter in duration than a normal interaction. We could build a table with duration statistics for each type of interaction in our dataset. First we need to get a list of labels as described in the first line [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names).      ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "label_list = [\"back.\",\"buffer_overflow.\",\"ftp_write.\",\"guess_passwd.\",\n              \"imap.\",\"ipsweep.\",\"land.\",\"loadmodule.\",\"multihop.\",\n              \"neptune.\",\"nmap.\",\"normal.\",\"perl.\",\"phf.\",\"pod.\",\"portsweep.\",\n              \"rootkit.\",\"satan.\",\"smurf.\",\"spy.\",\"teardrop.\",\"warezclient.\",\n              \"warezmaster.\"]", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Then we get a list of statistics for each label.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Get a list of stastics for each label\nstats_by_label = ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now we get the *duration* column, first in our dataset (i.e. index 0).  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Duration for each label\nduration_by_label = [ \n    (stat[0], np.array([float(stat[1].mean()[0]), float(sqrt(stat[1].variance()[0])), float(stat[1].min()[0]), float(stat[1].max()[0]), int(stat[1].count())])) \n    # Apply it for each label ]", "outputs": [], "metadata": {"collapsed": false}}, {"source": "That we can put into a Pandas data frame.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import pandas as pd\npd.set_option('display.max_columns', 50)\n\n# Create DF with columns [\"Mean\", \"Std Dev\", \"Min\", \"Max\", \"Count\"]\nstats_by_label_df = ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "And print it.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "print \"Duration statistics, by label\"\nstats_by_label_df", "outputs": [], "metadata": {"collapsed": false}}, {"source": "In order to reuse this code and get a dataframe from any variable in our dataset we will define a function.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def get_variable_stats_df(stats_by_label, column_i):\n    column_stats_by_label = [\n        (stat[0], np.array([float(stat[1].mean()[column_i]), float(sqrt(stat[1].variance()[column_i])), float(stat[1].min()[column_i]), float(stat[1].max()[column_i]), int(stat[1].count())])) \n        for stat in stats_by_label\n    ]\n    return pd.DataFrame.from_items(column_stats_by_label, columns=[\"Mean\", \"Std Dev\", \"Min\", \"Max\", \"Count\"], orient='index')", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let's try for *duration* again.   ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "get_variable_stats_df(stats_by_label,0)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now for the next numeric column in the dataset, *src_bytes*.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "print \"src_bytes statistics, by label\"\nget_variable_stats_df(stats_by_label,1)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "And so on. By reusing the `summary_by_label` and `get_variable_stats_df` functions we can perform some exploratory data analysis in large datasets with Spark.  ", "cell_type": "markdown", "metadata": {}}, {"source": "## Correlations", "cell_type": "markdown", "metadata": {}}, {"source": "Spark's MLlib supports [Pearson\u2019s](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) and [Spearman\u2019s](http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) to calculate pairwise correlation methods among many series. Both of them are provided by the `corr` method in the `Statistics` package.  ", "cell_type": "markdown", "metadata": {}}, {"source": "We have two options as input. Either two `RDD[Double]`s or an `RDD[Vector]`. In the first case the output will be a `Double` value, while in the second a whole correlation Matrix. Due to the nature of our data, we will obtain the second.    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.mllib.stat import Statistics \n# Create a correlation matrix with the vector_data and `spearman`\ncorrelation_matrix =", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Once we have the correlations ready, we can start inspecting their values.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import pandas as pd\npd.set_option('display.max_columns', 50)\n\ncol_names = [\"duration\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\n             \"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n             \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n             \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n             \"is_hot_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n             \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n             \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n             \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n             \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n             \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n\n# Create a DF of correlation_matrix, with index as col_names and columns as col_names\ncorr_df =\n\ncorr_df", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We have used a *Pandas* `DataFrame` here to render the correlation matrix in a more comprehensive way. Now we want those variables that are highly correlated. For that we do a bit of dataframe manipulation.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# get a boolean dataframe where true means that a pair of variables is highly correlated (between 0.8 and 1)\nhighly_correlated_df = \n# get the names of the variables so we can use them to slice the dataframe\ncorrelated_vars_index = \ncorrelated_var_names = \n# slice it\nhighly_correlated_df.loc[correlated_var_names,correlated_var_names]", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Conclusions and possible model selection hints", "cell_type": "markdown", "metadata": {}}, {"source": "The previous dataframe showed us which variables are highly correlated. We have kept just those variables with at least one strong correlation. We can use as we please, but a good way could be to do some model selection. That is, if we have a group of variables that are highly correlated, we can keep just one of them to represent the group under the assumption that they convey similar information as predictors. Reducing the number of variables will not improve our model accuracy, but it will make it easier to understand and also more efficient to compute.  ", "cell_type": "markdown", "metadata": {}}, {"source": "For example, from the description of the [KDD Cup 99 task](http://kdd.ics.uci.edu/databases/kddcup99/task.html) we know that the variable `dst_host_same_src_port_rate` references the percentage of the last 100 connections to the same port, for the same destination host. In our correlation matrix (and auxiliar dataframes) we find that this one is highly and positively correlated to `src_bytes` and `srv_count`. The former is the number of bytes sent form source to destination. The later is the number of connections to the same service as the current connection in the past 2 seconds. We might decide not to include `dst_host_same_src_port_rate` in our model if we include the other two, as a way to reduce the number of variables and later one better interpret our models.  ", "cell_type": "markdown", "metadata": {}}, {"source": "Later on, in those notebooks dedicated to build predictive models, we will make use of this information to build more interpretable models.   ", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}