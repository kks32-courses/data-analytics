{"nbformat_minor": 2, "cells": [{"execution_count": null, "cell_type": "code", "source": "# Upload dataset to `/user/livy/kddcup.data_10_percent.gz`\ndata_file =\n# Create an RDD from the KDD99 10% dataset\nraw_data = ", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Count the number of lines in the raw dataset\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Print first few entries\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Filter `normal` data\nnormal_raw_data =", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Count the normal data and measure time\nfrom time import time\nt0 = time()\ntt = time() - t0\nprint (\"There are {} 'normal' interactions\".format(normal_count))\nprint (\"Count completed in {} seconds\".format(round(tt,3)))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Sample data\nraw_data_sample = \nsample_size = \ntotal_size = \nprint (\"Sample size is {} of {}\".format(sample_size, total_size))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Measuring the normal interaction in the sample dataset\nfrom time import time\n\n# transformations to get normal data\n# raw data split csv\nraw_data_sample_items = \n# filter normal data\nsample_normal_tags = \n\n# actions + time\nt0 = time()\nsample_normal_tags_count = \ntt = time() - t0\n\n# Compute the ratio\nsample_normal_ratio = \nprint (\"The ratio of 'normal' interactions is {}\".format(round(sample_normal_ratio,3)))\nprint (\"Count done in {} seconds\".format(round(tt,3)))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Measuring the normal interaction in the entire dataset\n# raw csv\nraw_data_items = \n# filtered normal data\nnormal_tags = \n\n# actions + time\nt0 = time()\nnormal_tags_count = \ntt = time() - t0\n\nnormal_ratio = \nprint (\"The ratio of 'normal' interactions is {}\".format(round(sample_normal_ratio,3)))\nprint (\"Count done in {} seconds\".format(round(tt,3)))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Subtract the normal data from the entire dataset to get attacks\nattack_raw_data =\nprint (\"There are {} attack interactions\".format(round(attack_raw_data.count(),3)))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Extract protocols\n# Isolate each collection of values in two separate RDDs. \n# For that we will use distinct on the CSV-parsed dataset. \n# From the dataset description we know that protocol is the \n# second column and service is the third (tag is the last \n# one and not the first as appears in the page).\n\n# Split csv by `,`\ncsv_data = \n# Get distinct protocols in the second column\nprotocols = \nprotocols.collect()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Extract services in the third column\nservices = \nservices.collect()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Print all possible combinations of protocols x services\nproduct =\nprint (\"There are {} combinations of protocol X service\".format(len(product)))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# filter normal and attack RDDs using interaction type defined in `x[41]`\nnormal_csv_data = \nattack_csv_data = ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Duration of normal and attack (convert to integer)\nnormal_duration_data = \nattack_duration_data = ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Total duration of normal and attack (using reduce)\ntotal_normal_duration = \ntotal_attack_duration = \n\nprint (\"Total duration for 'normal' interactions is {}\".format(total_normal_duration))\nprint (\"Total duration for 'attack' interactions is {}\".format(total_attack_duration))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Mean duration of normal and attack\nnormal_count = \nattack_count = \n\nprint (\"Mean duration for 'normal' interactions is {}\".format(round(total_normal_duration/float(normal_count),3)))\nprint (\"Mean duration for 'attack' interactions is {}\".format(round(total_attack_duration/float(attack_count),3)))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Using aggregate\n# The aggregate action does not require the return type to be the same type as the RDD. \n# Like with fold, we supply an initial zero value of the type we want to return. Then we provide two functions.\n# The first one is used to combine the elements from our RDD with the accumulator. \n# The second function is needed to merge two accumulators.\n\n# Normal interactions `.aggregate(initial_value, combine value with accumulators, combine accumulators)`\nnormal_sum_count = \n\nprint (\"Mean duration for 'normal' interactions is {}\".\\\n       format(round(normal_sum_count[0]/float(normal_sum_count[1]),3)))\n\n# Attack interactions `.aggregate(initial_value, combine value with accumulators, combine accumulators)`\nattack_sum_count = \n\nprint (\"Mean duration for 'attack' interactions is {}\".\\\n    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3)))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Key-value data\nkey_value_data = # x[41] contains the network interaction tag", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# The `reduceByKey()` transformation is used to calculate the total duration of each network interaction type.\nkey_value_duration = \ndurations_by_key = \ndurations_by_key.collect()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# counts by keys\ncounts_by_key = \ncounts_by_key", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# calculate per-type durations using combineByKey\n\nsum_counts = key_value_duration.combineByKey(\n    (??, # the initial value, with value x and count 1\n    (??, # how to combine a pair value with the accumulator: sum value, and increment count\n    (??) # combine accumulators\n)\n\nsum_counts.collectAsMap()", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}
