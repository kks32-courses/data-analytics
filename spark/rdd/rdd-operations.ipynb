{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "# Using map transformation\nnums = sc.parallelize([1, 2, 3, 4])\nsquared = nums.map(lambda x: x * x).collect()\nfor num in squared:\n    print(num)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1499169944872_0006</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-kbtu.lzixs4haezju5jpxr1e0wdmnla.ax.internal.cloudapp.net:8088/proxy/application_1499169944872_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.18:30060/node/containerlogs/container_1499169944872_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n1\n4\n9\n16"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# flatMap() in Python, splitting lines into words\nlines = sc.parallelize([\"hello world\", \"hi\"])\nwords = lines.flatMap(lambda line: line.split(\" \"))\nwords.first() # returns \"hello\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "'hello'"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# Print flatmap() output\nprint(words.take(words.count()))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['hello', 'world', 'hi']"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "# With map()\nwords = lines.map(lambda line: line.split(\" \"))\nprint(words.take(words.count()))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[['hello', 'world'], ['hi']]"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "# Create an RDD of {1, 2, 3, 3}\nrdd = sc.parallelize([1, 2, 3, 3])", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 7, "cell_type": "code", "source": "# map()\nmap = rdd.map(lambda x: x + 1)\nprint(map.take(map.count()))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[2, 3, 4, 4]"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "# flatMap()\nflatmap = rdd.flatMap(lambda x: range(x,4))\nprint(flatmap.take(flatmap.count()))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1, 2, 3, 2, 3, 3, 3]"}], "metadata": {"collapsed": false}}, {"execution_count": 9, "cell_type": "code", "source": "# filter()\nfilter = rdd.filter(lambda x: x != 1)\nprint(filter.take(filter.count()))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[2, 3, 3]"}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "# distinct()\ndistinct = rdd.distinct()\nprint(distinct.take(distinct.count()))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1, 2, 3]"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "# sample()\nsample = rdd.sample(False, 0.5)\nprint(sample.take(sample.count()))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[3]"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}