{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "# Create a RDD pair of {(1, 2), (3, 4), (3, 6)}\npairrdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])\npairrdd.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>14</td><td>application_1499674741924_0018</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-kbtu.3bs2jaxxb12u1ll00rolucu4sg.ax.internal.cloudapp.net:8088/proxy/application_1499674741924_0018/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.11:30060/node/containerlogs/container_1499674741924_0018_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n[(1, 2), (3, 4), (3, 6)]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# reduce by key\nreducebykey = pairrdd.reduceByKey(lambda x, y: x + y)\nreducebykey.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(1, 2), (3, 10)]"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# group by key\ngroupbykey = pairrdd.groupByKey()\ngroupbykey.map(lambda x : (x[0], list(x[1]))).collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(1, [2]), (3, [4, 6])]"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# map values\nmapvalues = pairrdd.mapValues(lambda x: x + 1)\nmapvalues.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(1, 3), (3, 5), (3, 7)]"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "# keys\nkeys = pairrdd.keys()\nkeys.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1, 3, 3]"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "# values\nvalues = pairrdd.values()\nvalues.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[2, 4, 6]"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "# sort by key\nsortbykey = pairrdd.sortByKey()\nsortbykey.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(1, 2), (3, 6), (3, 4)]"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "# other\nother = sc.parallelize([(3, 9)])\nother.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(3, 9)]"}], "metadata": {"collapsed": false}}, {"execution_count": 9, "cell_type": "code", "source": "# subtract\nsubtract = pairrdd.subtractByKey(other)\nsubtract.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(1, 2)]"}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "# join\njoin = pairrdd.join(other)\njoin.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(3, (4, 9)), (3, (6, 9))]"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}