{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "# Create an RDD from the KDD99 10% dataset\ndata_file = \"./kddcup.data_10_percent.gz\"\nraw_data = sc.textFile(data_file)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>15</td><td>application_1499674741924_0019</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-kbtu.3bs2jaxxb12u1ll00rolucu4sg.ax.internal.cloudapp.net:8088/proxy/application_1499674741924_0019/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.11:30060/node/containerlogs/container_1499674741924_0019_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# Count the number of lines in the raw dataset\nraw_data.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "494021"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# Print first few entries\nraw_data.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.', '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.', '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.', '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.', '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# Filter `normal` data\nnormal_raw_data = raw_data.filter(lambda x: 'normal.' in x)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "# Count the normal data and measure time\nfrom time import time\nt0 = time()\nnormal_count = normal_raw_data.count()\ntt = time() - t0\nprint (\"There are {} 'normal' interactions\".format(normal_count))\nprint (\"Count completed in {} seconds\".format(round(tt,3)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "There are 97278 'normal' interactions\nCount completed in 1.225 seconds"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "# Sample data\nraw_data_sample = raw_data.sample(False, 0.1, 1234)\nsample_size = raw_data_sample.count()\ntotal_size = raw_data.count()\nprint (\"Sample size is {} of {}\".format(sample_size, total_size))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Sample size is 49493 of 494021"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "# Measuring the normal interaction in the sample dataset\nfrom time import time\n\n# transformations to get normal data\nraw_data_sample_items = raw_data_sample.map(lambda x: x.split(\",\"))\nsample_normal_tags = raw_data_sample_items.filter(lambda x: \"normal.\" in x)\n\n# actions + time\nt0 = time()\nsample_normal_tags_count = sample_normal_tags.count()\ntt = time() - t0\n\nsample_normal_ratio = sample_normal_tags_count / float(sample_size)\nprint (\"The ratio of 'normal' interactions is {}\".format(round(sample_normal_ratio,3)))\nprint (\"Count done in {} seconds\".format(round(tt,3)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The ratio of 'normal' interactions is 0.195\nCount done in 1.499 seconds"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "# Measuring the normal interaction in the entire dataset\nraw_data_items = raw_data.map(lambda x: x.split(\",\"))\nnormal_tags = raw_data_items.filter(lambda x: \"normal.\" in x)\n\n# actions + time\nt0 = time()\nnormal_tags_count = normal_tags.count()\ntt = time() - t0\n\nnormal_ratio = normal_tags_count / float(total_size)\nprint (\"The ratio of 'normal' interactions is {}\".format(round(sample_normal_ratio,3)))\nprint (\"Count done in {} seconds\".format(round(tt,3)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The ratio of 'normal' interactions is 0.195\nCount done in 2.763 seconds"}], "metadata": {"collapsed": false}}, {"source": "This shows that the normal interaction in the data set is 0.195 (from sampling and from the entire data set). The duration is about a second slower when operating on the entire data set.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "# Subtract the normal data from the entire dataset to get attacks\nattack_raw_data = raw_data.subtract(normal_raw_data)\nprint (\"There are {} attack interactions\".format(round(attack_raw_data.count(),3)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "There are 396743 attack interactions"}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "# Extract protocols\n# Isolate each collection of values in two separate RDDs. \n# For that we will use distinct on the CSV-parsed dataset. \n# From the dataset description we know that protocol is the \n# second column and service is the third (tag is the last \n# one and not the first as appears in the page).\ncsv_data = raw_data.map(lambda x: x.split(\",\"))\nprotocols = csv_data.map(lambda x: x[1]).distinct()\nprotocols.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['icmp', 'udp', 'tcp']"}], "metadata": {"collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "# Extract services\nservices = csv_data.map(lambda x: x[2]).distinct()\nservices.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['finger', 'http', 'netbios_dgm', 'name', 'hostnames', 'vmnet', 'systat', 'shell', 'netbios_ssn', 'urh_i', 'pop_3', 'ctf', 'domain', 'mtp', 'remote_job', 'exec', 'supdup', 'http_443', 'sunrpc', 'urp_i', 'pop_2', 'csnet_ns', 'smtp', 'whois', 'ldap', 'daytime', 'imap4', 'nntp', 'klogin', 'rje', 'IRC', 'link', 'eco_i', 'tftp_u', 'iso_tsap', 'uucp_path', 'auth', 'ecr_i', 'other', 'domain_u', 'courier', 'discard', 'red_i', 'tim_i', 'time', 'login', 'ftp', 'telnet', 'ntp_u', 'sql_net', 'echo', 'private', 'gopher', 'efs', 'netbios_ns', 'ftp_data', 'nnsp', 'ssh', 'netstat', 'uucp', 'Z39_50', 'kshell', 'X11', 'bgp', 'pm_dump', 'printer']"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "# Print all possible combinations of protocols x services\nproduct = protocols.cartesian(services).collect()\nprint (\"There are {} combinations of protocol X service\".format(len(product)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "There are 198 combinations of protocol X service"}], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "# separate normal and attack RDDs\nnormal_csv_data = csv_data.filter(lambda x: x[41]==\"normal.\")\nattack_csv_data = csv_data.filter(lambda x: x[41]!=\"normal.\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 14, "cell_type": "code", "source": "# Duration of normal and attack\nnormal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\nattack_duration_data = attack_csv_data.map(lambda x: int(x[0]))", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 15, "cell_type": "code", "source": "# Total duration of normal and attack\ntotal_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\ntotal_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n\nprint (\"Total duration for 'normal' interactions is {}\".format(total_normal_duration))\nprint (\"Total duration for 'attack' interactions is {}\".format(total_attack_duration))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Total duration for 'normal' interactions is 21075991\nTotal duration for 'attack' interactions is 2626792"}], "metadata": {"collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "# Mean duration of normal and attack\nnormal_count = normal_duration_data.count()\nattack_count = attack_duration_data.count()\n\nprint (\"Mean duration for 'normal' interactions is {}\".format(round(total_normal_duration/float(normal_count),3)))\nprint (\"Mean duration for 'attack' interactions is {}\".format(round(total_attack_duration/float(attack_count),3)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Mean duration for 'normal' interactions is 216.657\nMean duration for 'attack' interactions is 6.621"}], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "# Using aggregate\n# The aggregate action does not require the return type to be the same type as the RDD. \n# Like with fold, we supply an initial zero value of the type we want to return. Then we provide two functions.\n# The first one is used to combine the elements from our RDD with the accumulator. \n# The second function is needed to merge two accumulators.\n\n# Normal interactions\nnormal_sum_count = normal_duration_data.aggregate(\n    (0,0), # the initial value\n    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n)\n\nprint (\"Mean duration for 'normal' interactions is {}\".\\\n       format(round(normal_sum_count[0]/float(normal_sum_count[1]),3)))\n\n# Attack interactions\nattack_sum_count = attack_duration_data.aggregate(\n    (0,0), # the initial value\n    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n)\n\nprint (\"Mean duration for 'attack' interactions is {}\".\\\n    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Mean duration for 'normal' interactions is 216.657\nMean duration for 'attack' interactions is 6.621"}], "metadata": {"collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "# Key-value data\nkey_value_data = csv_data.map(lambda x: (x[41], x)) # x[41] contains the network interaction tag", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 19, "cell_type": "code", "source": "# The `reduceByKey()` transformation is used to calculate the total duration of each network interaction type.\nkey_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) \ndurations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)\ndurations_by_key.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('portsweep.', 1991911.0), ('neptune.', 0.0), ('satan.', 64.0), ('pod.', 0.0), ('multihop.', 1288.0), ('back.', 284.0), ('warezclient.', 627563.0), ('nmap.', 0.0), ('smurf.', 0.0), ('guess_passwd.', 144.0), ('ftp_write.', 259.0), ('imap.', 72.0), ('land.', 0.0), ('loadmodule.', 326.0), ('buffer_overflow.', 2751.0), ('perl.', 124.0), ('ipsweep.', 43.0), ('rootkit.', 1008.0), ('phf.', 18.0), ('teardrop.', 0.0), ('warezmaster.', 301.0), ('normal.', 21075991.0), ('spy.', 636.0)]"}], "metadata": {"collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": "# counts by keys\ncounts_by_key = key_value_data.countByKey()\ncounts_by_key", "outputs": [{"output_type": "stream", "name": "stdout", "text": "defaultdict(<class 'int'>, {'neptune.': 107201, 'spy.': 2, 'normal.': 97278, 'guess_passwd.': 53, 'multihop.': 7, 'satan.': 1589, 'loadmodule.': 9, 'warezclient.': 1020, 'rootkit.': 10, 'ftp_write.': 8, 'back.': 2203, 'perl.': 3, 'smurf.': 280790, 'pod.': 264, 'land.': 21, 'warezmaster.': 20, 'nmap.': 231, 'portsweep.': 1040, 'buffer_overflow.': 30, 'teardrop.': 979, 'ipsweep.': 1247, 'imap.': 12, 'phf.': 4})"}], "metadata": {"collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": "# calculate per-type durations using combineByKey\n\nsum_counts = key_value_duration.combineByKey(\n    (lambda x: (x, 1)), # the initial value, with value x and count 1\n    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n)\n\nsum_counts.collectAsMap()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "{'neptune.': (0.0, 107201), 'spy.': (636.0, 2), 'normal.': (21075991.0, 97278), 'guess_passwd.': (144.0, 53), 'multihop.': (1288.0, 7), 'satan.': (64.0, 1589), 'loadmodule.': (326.0, 9), 'warezclient.': (627563.0, 1020), 'rootkit.': (1008.0, 10), 'ftp_write.': (259.0, 8), 'back.': (284.0, 2203), 'perl.': (124.0, 3), 'smurf.': (0.0, 280790), 'pod.': (0.0, 264), 'land.': (0.0, 21), 'warezmaster.': (301.0, 20), 'nmap.': (0.0, 231), 'portsweep.': (1991911.0, 1040), 'buffer_overflow.': (2751.0, 30), 'teardrop.': (0.0, 979), 'ipsweep.': (43.0, 1247), 'imap.': (72.0, 12), 'phf.': (18.0, 4)}"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}